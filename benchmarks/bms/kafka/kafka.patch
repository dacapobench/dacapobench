diff -ur ./kafka-3.3.1-src/config/log4j.properties ../build/kafka-3.3.1-src/config/log4j.properties
--- ./kafka-3.3.1-src/config/log4j.properties	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/config/log4j.properties	2024-04-26 02:56:10.305554934 +0000
@@ -14,8 +14,8 @@
 # limitations under the License.
 
 # Unspecified loggers and loggers with additivity=true output to server.log and stdout
-# Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise
-log4j.rootLogger=INFO, stdout, kafkaAppender
+# Note that ERROR only applies to unspecified loggers, the log level of the child logger is used otherwise
+log4j.rootLogger=ERROR, stdout, kafkaAppender
 
 log4j.appender.stdout=org.apache.log4j.ConsoleAppender
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
@@ -58,34 +58,34 @@
 log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n
 
 # Change the line below to adjust ZK client logging
-log4j.logger.org.apache.zookeeper=INFO
+log4j.logger.org.apache.zookeeper=ERROR
 
 # Change the two lines below to adjust the general broker logging level (output to server.log and stdout)
-log4j.logger.kafka=INFO
-log4j.logger.org.apache.kafka=INFO
+log4j.logger.kafka=ERROR
+log4j.logger.org.apache.kafka=ERROR
 
-# Change to DEBUG or TRACE to enable request logging
-log4j.logger.kafka.request.logger=WARN, requestAppender
+# Change to DEBUG or ERROR to enable request logging
+log4j.logger.kafka.request.logger=ERROR, requestAppender
 log4j.additivity.kafka.request.logger=false
 
-# Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output
+# Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to ERROR for additional output
 # related to the handling of requests
-#log4j.logger.kafka.network.Processor=TRACE, requestAppender
-#log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender
+#log4j.logger.kafka.network.Processor=ERROR, requestAppender
+#log4j.logger.kafka.server.KafkaApis=ERROR, requestAppender
 #log4j.additivity.kafka.server.KafkaApis=false
-log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender
+log4j.logger.kafka.network.RequestChannel$=ERROR, requestAppender
 log4j.additivity.kafka.network.RequestChannel$=false
 
-log4j.logger.kafka.controller=TRACE, controllerAppender
+log4j.logger.kafka.controller=ERROR, controllerAppender
 log4j.additivity.kafka.controller=false
 
-log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender
+log4j.logger.kafka.log.LogCleaner=ERROR, cleanerAppender
 log4j.additivity.kafka.log.LogCleaner=false
 
-log4j.logger.state.change.logger=INFO, stateChangeAppender
+log4j.logger.state.change.logger=ERROR, stateChangeAppender
 log4j.additivity.state.change.logger=false
 
-# Access denials are logged at INFO level, change to DEBUG to also log allowed accesses
-log4j.logger.kafka.authorizer.logger=INFO, authorizerAppender
+# Access denials are logged at ERROR level, change to DEBUG to also log allowed accesses
+log4j.logger.kafka.authorizer.logger=ERROR, authorizerAppender
 log4j.additivity.kafka.authorizer.logger=false
 
diff -ur ./kafka-3.3.1-src/config/server.properties ../build/kafka-3.3.1-src/config/server.properties
--- ./kafka-3.3.1-src/config/server.properties	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/config/server.properties	2024-04-26 02:58:22.316218739 +0000
@@ -59,7 +59,7 @@
 ############################# Log Basics #############################
 
 # A comma separated list of directories under which to store log files
-log.dirs=/tmp/kafka-logs
+log.dirs=scratch/kafka-logs
 
 # The default number of log partitions per topic. More partitions allow greater
 # parallelism for consumption, but this will also result in more files across
@@ -114,6 +114,7 @@
 # The interval at which log segments are checked to see if they can be deleted according
 # to the retention policies
 log.retention.check.interval.ms=300000
+log.segment.delete.delay.ms = 10
 
 ############################# Zookeeper #############################
 
diff -ur ./kafka-3.3.1-src/config/tools-log4j.properties ../build/kafka-3.3.1-src/config/tools-log4j.properties
--- ./kafka-3.3.1-src/config/tools-log4j.properties	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/config/tools-log4j.properties	2024-04-26 02:56:10.305554934 +0000
@@ -13,7 +13,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-log4j.rootLogger=WARN, stderr
+log4j.rootLogger=ERROR, stderr
 
 log4j.appender.stderr=org.apache.log4j.ConsoleAppender
 log4j.appender.stderr.layout=org.apache.log4j.PatternLayout
diff -ur ./kafka-3.3.1-src/config/zookeeper.properties ../build/kafka-3.3.1-src/config/zookeeper.properties
--- ./kafka-3.3.1-src/config/zookeeper.properties	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/config/zookeeper.properties	2024-04-26 02:56:10.305554934 +0000
@@ -13,7 +13,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # the directory where the snapshot is stored.
-dataDir=/tmp/zookeeper
+dataDir=./scratch/zookeeper
 # the port at which the clients will connect
 clientPort=2181
 # disable the per-ip limit on the number of connections since this is a non-production config
diff -ur ./kafka-3.3.1-src/core/src/main/scala/kafka/admin/TopicCommand.scala ../build/kafka-3.3.1-src/core/src/main/scala/kafka/admin/TopicCommand.scala
--- ./kafka-3.3.1-src/core/src/main/scala/kafka/admin/TopicCommand.scala	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/core/src/main/scala/kafka/admin/TopicCommand.scala	2024-04-26 02:56:10.305554934 +0000
@@ -73,7 +73,8 @@
         exitCode = 1
     } finally {
       topicService.close()
-      Exit.exit(exitCode)
+      return
+      //Exit.exit(exitCode)
     }
   }
 
diff -ur ./kafka-3.3.1-src/core/src/main/scala/kafka/log/LogManager.scala ../build/kafka-3.3.1-src/core/src/main/scala/kafka/log/LogManager.scala
--- ./kafka-3.3.1-src/core/src/main/scala/kafka/log/LogManager.scala	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/core/src/main/scala/kafka/log/LogManager.scala	2024-04-26 02:56:10.305554934 +0000
@@ -76,7 +76,7 @@
   import LogManager._
 
   val LockFile = ".lock"
-  val InitialTaskDelayMs = 30 * 1000
+  val InitialTaskDelayMs = 10
 
   private val logCreationOrDeletionLock = new Object
   private val currentLogs = new Pool[TopicPartition, UnifiedLog]()
diff -ur ./kafka-3.3.1-src/core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala ../build/kafka-3.3.1-src/core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala
--- ./kafka-3.3.1-src/core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala	2024-04-26 02:56:10.305554934 +0000
@@ -237,7 +237,7 @@
         Some(Utils.loadProps(absolutePath))
       } catch {
         case _: NoSuchFileException =>
-          warn(s"No meta.properties file under dir $absolutePath")
+          // warn(s"No meta.properties file under dir $absolutePath")
           None
         case e: Exception =>
           error(s"Failed to read meta.properties file under dir $absolutePath", e)
diff -ur ./kafka-3.3.1-src/tests/spec/simple_produce_bench.json ../build/kafka-3.3.1-src/tests/spec/simple_produce_bench.json
--- ./kafka-3.3.1-src/tests/spec/simple_produce_bench.json	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/tests/spec/simple_produce_bench.json	2024-04-26 02:56:10.309554893 +0000
@@ -1,40 +1,22 @@
-// Licensed to the Apache Software Foundation (ASF) under one or more
-// contributor license agreements.  See the NOTICE file distributed with
-// this work for additional information regarding copyright ownership.
-// The ASF licenses this file to You under the Apache License, Version 2.0
-// (the "License"); you may not use this file except in compliance with
-// the License.  You may obtain a copy of the License at
-//
-//    http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-//
-// An example task specification for running a producer benchmark in Trogdor.
-// See TROGDOR.md for details.
-//
-
 {
   "class": "org.apache.kafka.trogdor.workload.ProduceBenchSpec",
   "durationMs": 10000000,
   "producerNode": "node0",
+  "producers": %s,
   "bootstrapServers": "localhost:9092",
-  "targetMessagesPerSec": 10000,
-  "maxMessages": 50000,
+  "maxMessages": %s,
+  "producerConf": {
+    "linger.ms": 0
+  },
   "activeTopics": {
-    "foo[1-3]": {
-      "numPartitions": 10,
+    "dacapo": {
+      "numPartitions": %s,
       "replicationFactor": 1
     }
   },
-  "inactiveTopics": {
-    "foo[4-5]": {
-      "numPartitions": 10,
-      "replicationFactor": 1
-    }
-  }
-}
+    "valueGenerator": {
+        "type": "constant",
+        "size": %s
+    },
+  "messagesPerFlush": %s
+}
\ No newline at end of file
diff -ur ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/agent/Agent.java ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/agent/Agent.java
--- ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/agent/Agent.java	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/agent/Agent.java	2024-04-26 03:01:50.682231840 +0000
@@ -187,16 +187,16 @@
             e.printStackTrace(out);
             return false;
         }
-        out.println("Waiting for completion of task:" + JsonUtil.toPrettyJsonString(spec));
+        // out.println("Waiting for completion of task:" + JsonUtil.toPrettyJsonString(spec));
         String error = future.get();
         if (error == null || error.isEmpty()) {
-            out.println("Task succeeded with status " +
-                JsonUtil.toPrettyJsonString(workerManager.workerStates().get(EXEC_WORKER_ID).status()));
+            // out.println("Task succeeded with status " +
+            //    JsonUtil.toPrettyJsonString(workerManager.workerStates().get(EXEC_WORKER_ID).status()));
             return true;
         } else {
-            out.println("Task failed with status " +
-                JsonUtil.toPrettyJsonString(workerManager.workerStates().get(EXEC_WORKER_ID).status()) +
-                " and error " + error);
+            // out.println("Task failed with status " +
+            //   JsonUtil.toPrettyJsonString(workerManager.workerStates().get(EXEC_WORKER_ID).status()) +
+            //    " and error " + error);
             return false;
         }
     }
@@ -250,7 +250,7 @@
         final Agent agent = new Agent(platform, Scheduler.SYSTEM, restServer, resource);
         restServer.start(resource);
         Exit.addShutdownHook("agent-shutdown-hook", () -> {
-            log.warn("Running agent shutdown hook.");
+            // log.warn("Running agent shutdown hook.");
             try {
                 agent.beginShutdown();
                 agent.waitForShutdown();
@@ -265,10 +265,14 @@
             } catch (Exception e) {
                 System.out.println("Unable to parse the supplied task spec.");
                 e.printStackTrace();
-                Exit.exit(1);
+                agent.beginShutdown();                 
+                //Exit.exit(1);
             }
             TaskSpec effectiveSpec = agent.rebaseTaskSpecTime(spec);
-            Exit.exit(agent.exec(effectiveSpec, System.out) ? 0 : 1);
+            agent.exec(effectiveSpec, System.out);            
+            agent.beginShutdown();
+            //Remove Exit.exit because the agent is executed in the Dacapo thread
+            //Exit.exit(agent.exec(effectiveSpec, System.out) ? 0 : 1);
         }
         agent.waitForShutdown();
     }
diff -ur ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/common/WorkerUtils.java ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/common/WorkerUtils.java
--- ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/common/WorkerUtils.java	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/common/WorkerUtils.java	2024-04-26 03:02:43.809743696 +0000
@@ -106,6 +106,7 @@
 
     private static final int ADMIN_REQUEST_TIMEOUT = 25000;
     private static final int CREATE_TOPICS_CALL_TIMEOUT = 180000;
+    private static final int CREATE_TOPICS_CALL_WAITTIME = 100;
     private static final int MAX_CREATE_TOPICS_BATCH_SIZE = 10;
 
             //Map<String, Map<Integer, List<Integer>>> topics) throws Throwable {
@@ -180,6 +181,7 @@
         long startMs = Time.SYSTEM.milliseconds();
         int tries = 0;
         List<String> existingTopics = new ArrayList<>();
+        boolean existingDeleted = false;
 
         Map<String, NewTopic> newTopics = new HashMap<>();
         for (NewTopic newTopic : topics) {
@@ -214,6 +216,7 @@
                         topicsToCreate.add(topicName);
                     } else if (e.getCause() instanceof TopicExistsException) {
                         log.info("Topic {} already exists.", topicName);
+                        topicsToCreate.add(topicName);
                         existingTopics.add(topicName);
                     } else {
                         log.warn("Failed to create {}", topicName, e.getCause());
@@ -224,12 +227,17 @@
             if (topicsToCreate.isEmpty()) {
                 break;
             }
+            if (!existingTopics.isEmpty() && !existingDeleted) {
+                adminClient.deleteTopics(existingTopics);
+                existingDeleted = true;
+            }
             if (Time.SYSTEM.milliseconds() > startMs + CREATE_TOPICS_CALL_TIMEOUT) {
                 String str = "Unable to create topic(s): " +
                              Utils.join(topicsToCreate, ", ") + "after " + tries + " attempt(s)";
                 log.warn(str);
                 throw new TimeoutException(str);
             }
+            Thread.sleep(CREATE_TOPICS_CALL_WAITTIME);
         }
         return existingTopics;
     }
diff -ur ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchSpec.java ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchSpec.java
--- ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchSpec.java	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchSpec.java	2024-04-26 02:56:10.309554893 +0000
@@ -60,6 +60,7 @@
  */
 public class ProduceBenchSpec extends TaskSpec {
     private final String producerNode;
+    private final int producers;
     private final String bootstrapServers;
     private final int targetMessagesPerSec;
     private final long maxMessages;
@@ -73,11 +74,13 @@
     private final TopicsSpec inactiveTopics;
     private final boolean useConfiguredPartitioner;
     private final boolean skipFlush;
+    private final int messagesPerFlush;
 
     @JsonCreator
     public ProduceBenchSpec(@JsonProperty("startMs") long startMs,
                          @JsonProperty("durationMs") long durationMs,
                          @JsonProperty("producerNode") String producerNode,
+                         @JsonProperty("producers") int producers,
                          @JsonProperty("bootstrapServers") String bootstrapServers,
                          @JsonProperty("targetMessagesPerSec") int targetMessagesPerSec,
                          @JsonProperty("maxMessages") long maxMessages,
@@ -90,9 +93,11 @@
                          @JsonProperty("activeTopics") TopicsSpec activeTopics,
                          @JsonProperty("inactiveTopics") TopicsSpec inactiveTopics,
                          @JsonProperty("useConfiguredPartitioner") boolean useConfiguredPartitioner, 
-                         @JsonProperty("skipFlush") boolean skipFlush) {
+                         @JsonProperty("skipFlush") boolean skipFlush,
+                         @JsonProperty("messagesPerFlush") int messagesPerFlush) {
         super(startMs, durationMs);
         this.producerNode = (producerNode == null) ? "" : producerNode;
+        this.producers = producers == 0? 1: producers;
         this.bootstrapServers = (bootstrapServers == null) ? "" : bootstrapServers;
         this.targetMessagesPerSec = targetMessagesPerSec;
         this.maxMessages = maxMessages;
@@ -110,6 +115,7 @@
             TopicsSpec.EMPTY : inactiveTopics.immutableCopy();
         this.useConfiguredPartitioner = useConfiguredPartitioner;
         this.skipFlush = skipFlush;
+        this.messagesPerFlush = messagesPerFlush;
     }
 
     @JsonProperty
@@ -118,6 +124,11 @@
     }
 
     @JsonProperty
+    public int producers() {
+      return producers;
+    }
+
+    @JsonProperty
     public String bootstrapServers() {
         return bootstrapServers;
     }
@@ -182,6 +193,11 @@
         return skipFlush;
     }
 
+    @JsonProperty
+    public int messagesPerFlush() {
+      return messagesPerFlush;
+    }
+
     @Override
     public TaskController newController(String id) {
         return topology -> Collections.singleton(producerNode);
diff -ur ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchWorker.java ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchWorker.java
--- ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchWorker.java	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/ProduceBenchWorker.java	2024-04-26 03:05:57.452011629 +0000
@@ -55,6 +55,9 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 
+import java.lang.reflect.Method;
+import java.lang.reflect.InvocationTargetException;
+
 public class ProduceBenchWorker implements TaskWorker {
     private static final Logger log = LoggerFactory.getLogger(ProduceBenchWorker.class);
     
@@ -72,6 +75,11 @@
 
     private KafkaFutureImpl<String> doneFuture;
 
+    Method dacapoStart;
+    Method dacapoEnd;
+    Method dacapoStarting;
+    Method dacapoFinished;
+
     public ProduceBenchWorker(String id, ProduceBenchSpec spec) {
         this.id = id;
         this.spec = spec;
@@ -93,6 +101,26 @@
         executor.submit(new Prepare());
     }
 
+    public void dacapoStarting() {
+        try {
+            dacapoStarting.invoke(null);
+        } catch (IllegalAccessException e) {
+            System.err.println("Failed to invoke DaCapo latency reporter requestsStarting(): "+e);
+        } catch (InvocationTargetException e) {
+            System.err.println("Failed to invoke DaCapo latency reporter requestsStarting(): "+e);
+        }
+    }
+
+    public void dacapoFinished() {
+        try {
+            dacapoFinished.invoke(null);
+        } catch (IllegalAccessException e) {
+            System.err.println("Failed to invoke DaCapo latency reporter requestsFinished(): "+e);
+        } catch (InvocationTargetException e) {
+            System.err.println("Failed to invoke DaCapo latency reporter requestsFinished(): "+e);
+        }
+    }
+
     public class Prepare implements Runnable {
         @Override
         public void run() {
@@ -119,8 +147,20 @@
                 }
                 status.update(new TextNode("Creating " + newTopics.keySet().size() + " topic(s)"));
                 WorkerUtils.createTopics(log, spec.bootstrapServers(), spec.commonClientConf(),
-                                         spec.adminClientConf(), newTopics, false);
+                    spec.adminClientConf(), newTopics, false);
                 status.update(new TextNode("Created " + newTopics.keySet().size() + " topic(s)"));
+                try {
+                    Class<?> clazz = Class.forName("org.dacapo.harness.LatencyReporter", true, ProduceBenchWorker.class.getClassLoader());
+                    dacapoStart = clazz.getMethod("start", int.class);
+                    dacapoEnd = clazz.getMethod("endIdx", int.class);
+                    dacapoStarting = clazz.getMethod("requestsStarting");
+                    dacapoFinished = clazz.getMethod("requestsFinished");
+                } catch (ClassNotFoundException e) {
+                    System.err.println("Failed to access DaCapo latency reporter: "+e);
+                } catch (NoSuchMethodException e) {
+                    System.err.println("Failed trying to create latency stats: "+e);
+                }
+                dacapoStarting();
                 executor.submit(new SendRecords(active));
             } catch (Throwable e) {
                 WorkerUtils.abort(log, "Prepare", e, doneFuture);
@@ -131,23 +171,57 @@
     private static class SendRecordsCallback implements Callback {
         private final SendRecords sendRecords;
         private final long startMs;
+        private Method dacapoEnd;
+        private Method dacapoStart;
+        private int idx;
 
-        SendRecordsCallback(SendRecords sendRecords, long startMs) {
+        SendRecordsCallback(SendRecords sendRecords, long startMs, Method dacapoEnd,  Method dacapoStart) {
             this.sendRecords = sendRecords;
             this.startMs = startMs;
+            this.dacapoEnd = dacapoEnd;
+            this.dacapoStart = dacapoStart;
+            this.idx = dacapoStart();
         }
 
         @Override
         public void onCompletion(RecordMetadata metadata, Exception exception) {
             long now = Time.SYSTEM.milliseconds();
             long durationMs = now - startMs;
+            dacapoEnd(idx);
             sendRecords.recordDuration(durationMs);
             if (exception != null) {
                 log.error("SendRecordsCallback: error", exception);
             }
         }
+        public int dacapoStart() {
+            int rtn = 0;
+            try {
+                rtn = (Integer) dacapoStart.invoke(null, 0);
+            } catch (IllegalAccessException e) {
+                System.err.println("Failed to invoke DaCapo latency reporter start(): "+e);
+                e.printStackTrace();
+            } catch (InvocationTargetException e) {
+                System.err.println("Failed to invoke DaCapo latency reporter start(): "+e);
+                e.printStackTrace();
+            }
+            return rtn;
+        }
+
+        public void dacapoEnd(int idx) {
+            try {
+                dacapoEnd.invoke(null, idx);
+            } catch (IllegalAccessException e) {
+                System.err.println("Failed to invoke DaCapo latency reporter end(): "+e);
+                e.printStackTrace();
+            } catch (InvocationTargetException e) {
+                System.err.println("Failed to invoke DaCapo latency reporter end(): "+e);
+                e.printStackTrace();
+            }
+        }
     }
 
+
+
     /**
      * A subclass of Throttle which flushes the Producer right before the throttle injects a delay.
      * This avoids including throttling latency in latency measurements.
@@ -159,14 +233,22 @@
             super(maxPerPeriod, THROTTLE_PERIOD_MS);
             this.producer = producer;
         }
+        SendRecordsThrottle(int maxPerPeriod, int throttle_period, KafkaProducer<?, ?> producer) {
+          super(maxPerPeriod, throttle_period);
+          this.producer = producer;
+      }
 
         @Override
         protected synchronized void delay(long amount) throws InterruptedException {
+          if (amount == -1) {
+            producer.flush();
+          } else {
             long startMs = time().milliseconds();
             producer.flush();
             long endMs = time().milliseconds();
             long delta = endMs - startMs;
             super.delay(amount - delta);
+          }
         }
     }
 
@@ -177,7 +259,11 @@
 
         private final Future<?> statusUpdaterFuture;
 
-        private final KafkaProducer<byte[], byte[]> producer;
+        // private final KafkaProducer<byte[],
+        private final KafkaProducer<byte[], byte[]> producers[];
+        private final Throttle throttles[];
+
+        private int currentProducer;
 
         private final PayloadIterator keys;
 
@@ -185,13 +271,14 @@
 
         private final Optional<TransactionGenerator> transactionGenerator;
 
-        private final Throttle throttle;
+        //private final Throttle throttle;
 
         private Iterator<TopicPartition> partitionsIterator;
         private Future<RecordMetadata> sendFuture;
         private AtomicLong transactionsCommitted;
         private boolean enableTransactions;
 
+        @SuppressWarnings("unchecked")
         SendRecords(HashSet<TopicPartition> activePartitions) {
             this.activePartitions = activePartitions;
             this.partitionsIterator = activePartitions.iterator();
@@ -211,14 +298,30 @@
                 props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "produce-bench-transaction-id-" + UUID.randomUUID());
             // add common client configs to producer properties, and then user-specified producer configs
             WorkerUtils.addConfigsToProperties(props, spec.commonClientConf(), spec.producerConf());
-            this.producer = new KafkaProducer<>(props, new ByteArraySerializer(), new ByteArraySerializer());
+            this.producers = (KafkaProducer<byte[], byte[]>[]) new KafkaProducer[spec.producers()];
+            //this.producers = (KafkaProducer<byte[], byte[]>[])  new ArrayList<KafkaProducer<byte[], byte[]>>(spec.producers()).toArray();        
+            this.throttles = new Throttle[spec.producers()];
+            for (int i=0; i<spec.producers(); i++) {
+              this.producers[i] = new KafkaProducer<>(props, new ByteArraySerializer(), new ByteArraySerializer());
+              if (spec.messagesPerFlush() > 0) {
+                if (spec.skipFlush()) {
+                  this.throttles[i] = new Throttle(spec.messagesPerFlush(), -1);
+                } else {
+                  this.throttles[i] = new SendRecordsThrottle(spec.messagesPerFlush(), -1, this.producers[i]);
+                }
+              } else {
+                if (spec.skipFlush()) {
+                    this.throttles[i] = new Throttle(perPeriod, THROTTLE_PERIOD_MS);
+                } else {
+                    this.throttles[i] = new SendRecordsThrottle(perPeriod, producers[i]);
+                }
+              }
+            }
+
             this.keys = new PayloadIterator(spec.keyGenerator());
             this.values = new PayloadIterator(spec.valueGenerator());
-            if (spec.skipFlush()) {
-                this.throttle = new Throttle(perPeriod, THROTTLE_PERIOD_MS);
-            } else {
-                this.throttle = new SendRecordsThrottle(perPeriod, producer);
-            }
+
+
         }
 
         @Override
@@ -226,8 +329,10 @@
             long startTimeMs = Time.SYSTEM.milliseconds();
             try {
                 try {
-                    if (enableTransactions)
-                        producer.initTransactions();
+                    if (enableTransactions) {
+                      for (int i=0; i<this.producers.length; i++) 
+                        producers[i].initTransactions();
+                    }
 
                     long sentMessages = 0;
                     while (sentMessages < spec.maxMessages()) {
@@ -242,8 +347,10 @@
                     if (enableTransactions)
                         takeTransactionAction(); // give the transactionGenerator a chance to commit if configured evenly
                 } catch (Exception e) {
-                    if (enableTransactions)
-                        producer.abortTransaction();
+                    if (enableTransactions) {
+                      for (int i=0; i<this.producers.length; i++)
+                        producers[i].abortTransaction();
+                    }
                     throw e;
                 } finally {
                     if (sendFuture != null) {
@@ -253,7 +360,8 @@
                             log.error("Exception on final future", e);
                         }
                     }
-                    producer.close();
+                    for (int i=0; i<this.producers.length; i++)
+                      producers[i].close();
                 }
             } catch (Exception e) {
                 WorkerUtils.abort(log, "SendRecords", e, doneFuture);
@@ -261,8 +369,8 @@
                 statusUpdaterFuture.cancel(false);
                 StatusData statusData = new StatusUpdater(histogram, transactionsCommitted).update();
                 long curTimeMs = Time.SYSTEM.milliseconds();
-                log.info("Sent {} total record(s) in {} ms.  status: {}",
-                    histogram.summarize().numSamples(), curTimeMs - startTimeMs, statusData);
+                log.info("Sent {} total record(s) in {} ms at rate {}/s.  status: {}",
+                    histogram.summarize().numSamples(), curTimeMs - startTimeMs, histogram.summarize().numSamples() * 1000.0 / (curTimeMs-startTimeMs), statusData);
             }
             doneFuture.complete("");
             return null;
@@ -274,16 +382,19 @@
             switch (nextAction) {
                 case BEGIN_TRANSACTION:
                     log.debug("Beginning transaction.");
-                    producer.beginTransaction();
+                    for (int i=0; i<this.producers.length; i++)              
+                      producers[i].beginTransaction();
                     break;
                 case COMMIT_TRANSACTION:
                     log.debug("Committing transaction.");
-                    producer.commitTransaction();
+                    for (int i=0; i<this.producers.length; i++)
+                      producers[i].commitTransaction();
                     transactionsCommitted.getAndIncrement();
                     break;
                 case ABORT_TRANSACTION:
                     log.debug("Aborting transaction.");
-                    producer.abortTransaction();
+                    for (int i=0; i<this.producers.length; i++)
+                      producers[i].abortTransaction();
                     break;
                 case NO_OP:
                     tookAction = false;
@@ -305,9 +416,12 @@
                 record = new ProducerRecord<>(
                     partition.topic(), partition.partition(), keys.next(), values.next());
             }
-            sendFuture = producer.send(record,
-                new SendRecordsCallback(this, Time.SYSTEM.milliseconds()));
-            throttle.increment();
+            sendFuture = producers[this.currentProducer].send(record,
+                new SendRecordsCallback(this, Time.SYSTEM.milliseconds(), dacapoEnd, dacapoStart));
+            throttles[this.currentProducer].increment();
+            this.currentProducer += 1;
+            if (this.currentProducer == this.producers.length)
+              this.currentProducer = 0;
         }
 
         void recordDuration(long durationMs) {
@@ -407,6 +521,8 @@
 
     @Override
     public void stop(Platform platform) throws Exception {
+        dacapoFinished();
+
         if (!running.compareAndSet(true, false)) {
             throw new IllegalStateException("ProduceBenchWorker is not running.");
         }
diff -ur ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java
--- ./kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java	2022-09-29 19:03:49.000000000 +0000
+++ ../build/kafka-3.3.1-src/trogdor/src/main/java/org/apache/kafka/trogdor/workload/Throttle.java	2024-04-26 02:56:10.309554893 +0000
@@ -41,6 +41,11 @@
                 count++;
                 return throttled;
             }
+            if (periodMs == -1) {
+              // we are not using the periodMs
+              delay(-1);
+              count = 0;
+            }
             lastTimeMs = time().milliseconds();
             long curPeriod = lastTimeMs / periodMs;
             if (curPeriod <= prevPeriod) {
Only in .: kafka.patch
